{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import gensim\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_path = {\n",
    "    'raw_data':'raw_data.csv',\n",
    "    'model_w2v':'w2v_model_50_V2',\n",
    "    'model_nn':'model_V2.json',\n",
    "    'model_nn_h5':'model_V2.h5',\n",
    "    'w2v_corpus':'train_corpus_V2',\n",
    "}\n",
    "\n",
    "para = {\n",
    "    'epoch': 300\n",
    "}\n",
    "class SemanticIntentionClassify():\n",
    "    \n",
    "    def __init__(self,model_path,para):\n",
    "        self.raw_data = pd.read_csv(model_path['raw_data'])\n",
    "        self.model_w2v = model_path['model_w2v']\n",
    "        self.model_nn = model_path['model_nn']\n",
    "        self.model_nn_h5 = model_path['model_nn_h5']\n",
    "        self.w2v_corpus = model_path['w2v_corpus']\n",
    "        self.epoch = para['epoch']\n",
    "\n",
    "    def w2v_corpus_generate(self):\n",
    "        \"\"\"词向量训练语聊生成\"\"\"\n",
    "        with open(self.w2v_corpus, 'a+') as f:\n",
    "            for sentence in list(self.raw_data['X']):\n",
    "                for char in sentence:\n",
    "                    f.write(char + ' ')\n",
    "        print ('finish!!')\n",
    "    \n",
    "    def _load_word2vec_model(self):\n",
    "        \"\"\"载入词向量模型和词的索引\"\"\"\n",
    "        \n",
    "        import gensim\n",
    "        model_w2v = gensim.models.Word2Vec.load(self.model_w2v)\n",
    "        i = 0\n",
    "        word_to_index = {}\n",
    "        for word in model_w2v.wv.vocab.keys():\n",
    "            word_to_index[word] = i\n",
    "            i += 1\n",
    "        return model_w2v,word_to_index\n",
    "    \n",
    "    def _load_nn_model(self):\n",
    "        \"\"\"载入预测模型\"\"\"\n",
    "        \n",
    "        from keras.models import model_from_json\n",
    "        json_file = open(self.model_nn, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        loaded_model.load_weights(self.model_nn_h5)\n",
    "        return loaded_model\n",
    "\n",
    "    def _sentences_to_indices(self,x, word_to_index, max_len):\n",
    "        \"\"\"批量句子转换为词向量字典对应的索引\"\"\"\n",
    "        \n",
    "        m = x.shape[0]\n",
    "        x_indices = np.zeros((m, max_len))\n",
    "        for i in range(m):\n",
    "            sentence_words = x[i]\n",
    "            j = 0\n",
    "            for w in sentence_words:\n",
    "                if w != ' ' and w in word_to_index.keys():\n",
    "                    x_indices[i, j] = word_to_index[w]\n",
    "                    j = j + 1\n",
    "        return x_indices\n",
    "\n",
    "    def data_generate(self):\n",
    "        \"\"\"\n",
    "        功能：\n",
    "        批量转化原始数据，用来预测或者训练\n",
    "        \n",
    "        用法：\n",
    "        model = SemanticIntentionClassify(model_path)\n",
    "        x,y = model.data_generate()\n",
    "        \"\"\"\n",
    "        \n",
    "        raw_data = self.raw_data\n",
    "        _,word_to_index = self._load_word2vec_model()\n",
    "        x = []\n",
    "        for index in raw_data.index:\n",
    "            x_raw = raw_data.iloc[index]['X']\n",
    "            sentence = []\n",
    "            for each in x_raw:\n",
    "                if each == ' ':\n",
    "                    continue\n",
    "                else:\n",
    "                    sentence.append(each)\n",
    "            x.append(sentence)\n",
    "        x = np.array(x)\n",
    "        x = self._sentences_to_indices(x, word_to_index, max_len=30)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(raw_data['Y'])\n",
    "        y_word_to_indice={}\n",
    "        for i in range(len(le.classes_)):   \n",
    "            y_word_to_indice[i] = le.classes_[i]\n",
    "        y = le.transform(raw_data['Y'])\n",
    "        return x,y,y_word_to_indice\n",
    "\n",
    "    def error_analysis(self,x,y):\n",
    "        \"\"\"\n",
    "        功能：\n",
    "        用于对输入的数据做误差检查，返回预测不对的数据索引\n",
    "        \n",
    "        参数：\n",
    "        x：方法data_generate生成的x\n",
    "        y：方法data_generate生成的y\n",
    "        \n",
    "        用法：\n",
    "        model = SemanticIntentionClassify(model_path)\n",
    "        x,y = model.data_generate()\n",
    "        fault_indice = model.error_analysis(x,y)\n",
    "        \"\"\"\n",
    "        \n",
    "        nn_model = self._load_nn_model()\n",
    "        model_w2v,_ = self._load_word2vec_model()\n",
    "        fault_indice = []\n",
    "        num = 0\n",
    "        for i in range(len(x)):\n",
    "            pred = nn_model.predict(np.array([x[i]]))\n",
    "            pred_y = np.argmax(pred)\n",
    "            if y[i] == pred_y:\n",
    "                num += 1\n",
    "            else:\n",
    "                fault_indice.append(i)\n",
    "        return fault_indice\n",
    "    \n",
    "    def predict(self,x,nn_model,model_w2v, word_to_index):\n",
    "        \"\"\"\n",
    "        功能：\n",
    "        用于预测句子的意图\n",
    "        \n",
    "        参数：\n",
    "        x：是要预测句子的字符串\n",
    "        \n",
    "        返回：\n",
    "        句子的意图\n",
    "        \n",
    "        用法：\n",
    "        model = SemanticIntentionClassify(model_path)\n",
    "        x = \"你的库里面有悦阅的部门吗？\"\n",
    "        num = model.error_analysis(x,y)\n",
    "        model.predict(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        #_,_,y_word_to_indice = self.data_generate()\n",
    "        y_word_to_indice = {0: '找职位', 1: '找邮箱', 2: '找部门', 3: '找领导'}\n",
    "        #nn_model = self._load_nn_model()\n",
    "        #model_w2v, word_to_index = self._load_word2vec_model()\n",
    "        inputs_list = []\n",
    "        for each in x:\n",
    "            if each in model_w2v.wv.vocab:\n",
    "                inputs_list.append(each)\n",
    "        x_test = np.array([inputs_list])        \n",
    "        x_test_indices = self._sentences_to_indices(x_test, word_to_index, 30)\n",
    "        pred = nn_model.predict(x_test_indices)\n",
    "        class_ = y_word_to_indice[np.argmax(pred)]\n",
    "        \n",
    "        return class_\n",
    "        \n",
    "    def train_w2v(self):\n",
    "        \"\"\"\n",
    "        作用：\n",
    "        训练语料的词向量模型\n",
    "        \n",
    "        例子：\n",
    "        model = SemanticIntentionClassify(model_path)\n",
    "        model_w2v = model.train_w2v()\n",
    "        model_w2v.save(\"{}\".format(path_of_model_you_want_to_save))\n",
    "        \"\"\"\n",
    "        from gensim.models import word2vec\n",
    "        from gensim import models\n",
    "        import gensim\n",
    "        import logging\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "        sentences = word2vec.Text8Corpus(self.w2v_corpus)\n",
    "        model = gensim.models.Word2Vec(size=50, window=5, min_count=1)\n",
    "        model.build_vocab(sentences)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=100)\n",
    "        model.save(\"{}\".format('w2v_model_50_V2'))\n",
    "        \n",
    "    def _pretrained_embedding_layer(self,word_to_vec_map, word_to_index):\n",
    "        \"\"\"\n",
    "        Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "        Arguments:\n",
    "        word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "        word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "        Returns:\n",
    "        embedding_layer -- pretrained layer Keras instance\n",
    "        \"\"\"\n",
    "    \n",
    "        vocab_len = len(word_to_index) + 1                 \n",
    "        emb_dim = word_to_vec_map['i'].shape[0]   \n",
    "        emb_matrix = np.zeros((vocab_len,emb_dim))\n",
    "        for word, index in word_to_index.items():\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        embedding_layer = Embedding(vocab_len,emb_dim,trainable=False)\n",
    "        embedding_layer.build((None,))\n",
    "        embedding_layer.set_weights([emb_matrix])\n",
    "        return embedding_layer\n",
    "        \n",
    "    def _keras_model(self, input_shape, word_to_vec_map, word_to_index):\n",
    "        \"\"\"\n",
    "        Function creating the Emojify-v2 model's graph.\n",
    "\n",
    "        Arguments:\n",
    "        input_shape -- shape of the input, usually (max_len,)\n",
    "        word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "        word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "        Returns:\n",
    "        model -- a model instance in Keras\n",
    "        \"\"\"\n",
    "\n",
    "        sentence_indices = Input(input_shape, dtype='int32')\n",
    "        embedding_layer = self._pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "        embeddings = embedding_layer(sentence_indices)   \n",
    "        X = LSTM(128, return_sequences=True)(embeddings)\n",
    "        X = Dropout(0.5)(X)\n",
    "        X = LSTM(128, return_sequences=False)(X)\n",
    "        X = Dropout(0.5)(X)\n",
    "        X = Dense(4)(X)\n",
    "        X = Activation('softmax')(X)\n",
    "        keras_model = Model(inputs=sentence_indices, outputs=X)\n",
    "        return keras_model\n",
    "    \n",
    "    def _embedding_dict(self,model_w2v):\n",
    "        embedding_dict = {}\n",
    "        for i in range(len(model_w2v.wv.vocab)):\n",
    "            embedding_vector = model_w2v.wv[model_w2v.wv.index2word[i]]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_dict[model_w2v.wv.index2word[i]] = embedding_vector\n",
    "        return embedding_dict\n",
    "    \n",
    "    def _convert_to_one_hot(self, y, c):\n",
    "        y = np.eye(c)[y.reshape(-1)]\n",
    "        return y\n",
    "    \n",
    "    def train_classifier(self):\n",
    "        \"\"\"\n",
    "        作用:\n",
    "        训练语义意图理解的分类器\n",
    "        \n",
    "        例子：\n",
    "        模型调用：\n",
    "        model = SemanticIntentionClassify(model_path,para)\n",
    "        model.train_classifier()\n",
    "        模型保存：\n",
    "        from keras.models import model_from_json\n",
    "        model_json = model.to_json()\n",
    "        with open(\"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(\"model.h5\")\n",
    "        \"\"\"\n",
    "        \n",
    "        x,y,_ = self.data_generate()\n",
    "        y = self._convert_to_one_hot(y, len(set(y)))\n",
    "        model_w2v,word_to_index = self._load_word2vec_model()\n",
    "        word_to_vec_map = self._embedding_dict(model_w2v)\n",
    "        maxLen = 30\n",
    "        model = self._keras_model((maxLen,), word_to_vec_map, word_to_index)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(x, y, epochs = self.epoch, shuffle=True)\n",
    "        from keras.models import model_from_json\n",
    "        model_json = model.to_json()\n",
    "        with open(\"model_V2.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(\"model_V2.h5\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SemanticIntentionClassify(model_path,para)\n",
    "#model.train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型实验：\n",
    "#### 通过人找找领导\n",
    "1. xxx的上司是谁？  \n",
    "fault_list：'张信', '曹信福', '谢子勃'\n",
    "\n",
    "2. 帮我找一下xxx的领导？  \n",
    "需要深度优化\n",
    "\n",
    "3. xxx的老大是谁啊？  \n",
    "fault_list: '王永新', '谢青卓', '谢正南', '王新', '谢毅舜', '谢妍', '张子荣', '邹子轩', '谢小平', '谢智', '贺子芮', '王子涵', '杨子亮', '李子振', '谢茂龙', '谢朋程', '谢丽胜', '王有圆', '卢发展', '谢敏', '谢名辉', '谢元珍', '王震新', '谢晓宇', '张信', '谢广振', '王会新', '谢志华', '要亚刚', '谢辉', '谢华强', '谢闻奇', '谢小霞', '谢红艳', '谢佳鑫', '曹信福', '谢东', '谢吉敬', '谢阳', '谢秋朋', '严润发', '谢子勃', '潘子龙', '郑子阳', '谢昆仑', '谢乔', '纪子寒', '刘子权', '蒲圆圆', '谢杰', '谢海鲸', '赵子斌', '金基勇', '谢婕', '谢国辉', '金子涵', '郭发军', '王子旭', '谢淑兰', '阮子扬', '谢时', '梁微子'\n",
    "\n",
    "4. 金基勇是{}上级吗?  \n",
    "fault_list: '王友明', '张子荣', '邹子轩', '贺子芮', '王子涵', '杨子亮', '张信', '曹信福', '谢子勃', '潘子龙', '侯明远', '安明明', '郑子阳', '纪子寒', '刘子权', '蒲圆圆', '赵子斌', '金子涵', '王子旭', '阮子扬'\n",
    "\n",
    "5. 如果你能找到那谁的部门经理，你就牛逼了  \n",
    "fault_list: '王子涵', '张信', '曹信福', '谢子勃', '蒲圆圆', '赵子斌', '金子涵', '王子旭']\n",
    "\n",
    "6. xxx在谁手底下干活  \n",
    "fault_list:  '路有人', '张子荣', '邹子轩', '贾月圆', '贺子芮', '王子涵', '杨子亮', '王有圆', '赵士芳', '张信', '要亚刚', '曹信福', '谢子勃', '潘子龙', '郑子阳', '纪子寒', '刘子权', '蒲圆圆', '王天一', '王立亚', '赵子斌', '金基勇', '金子涵', '王子旭', '李宝有', '阮子扬', '梁微子']\n",
    "\n",
    "7. xxx向谁报告工作？  \n",
    "fault_list: '张信', '曹信福', '谢子勃']\n",
    "\n",
    "8. xxx所在的部门经理能找到吗  \n",
    "fault_list: '张信', '曹信福']\n",
    "\n",
    "9. 找xxx汇报的认  \n",
    "fault_list: '张信', '曹信福', '谢子勃']\n",
    "\n",
    "10. 谁是xxx的直属经理?  \n",
    "\n",
    "11. '我不想找部门和职位信息，我想查崔月猛的领导是谁，你能行么?'  \n",
    "深度优化\n",
    "\n",
    "12. {}的所在部门的经理是谁？  \n",
    "fault_list: '张信', '曹信福', '谢子勃']\n",
    "\n",
    "#### 通过部门找领导\n",
    "1. xxx是谁负责的？\n",
    "\n",
    "2. xxx经理是谁？  \n",
    "fault_list: '电子发票与税务服务事业部', '电子发票与税务服务事业部-运营部']\n",
    "\n",
    "3. xxx的老大是谁?  \n",
    "fault_list: '职能管理组织审批-杜宇', '电子发票与税务服务事业部', 'NC电商资产产品部', 'NC智能制造实施组', '伙伴学院', 'NC智能制造研发组', '研发共享中心', '电商通', '职能管理组织审批-蔡治国', '大数据研发技术部', '职能管理组织审批-王健', '大数据解决方案创新部', '电子发票与税务服务事业部-运营部', '地产与交通公用事业部', '股份公司软件业务', '股份公司', '职能管理组织', '伙伴招募与发展部']\n",
    "\n",
    "4. xxx的部门经理是谁?   \n",
    "fault_list: '职能管理组织审批-杜宇', '电子发票与税务服务事业部', '职能管理组织审批-蔡治国', '职能管理组织审批-王健', '电子发票与税务服务事业部-运营部', '职能管理组织']\n",
    "\n",
    "5. xxx总经理是谁？  \n",
    "fault_list: '电子发票与税务服务事业部', '电子发票与税务服务事业部-运营部']\n",
    "\n",
    "#### 通过人找邮箱\n",
    "1. 能帮我找一下xxx的工作邮箱么？  \n",
    "深度优化\n",
    "\n",
    "2. 我想查找{}的电子邮箱？  \n",
    "fault_list: ['张信', '曹信福']\n",
    "\n",
    "3. 我要给{}发给邮件，地址是多少？  \n",
    "fault_list: ['张信', '曹信福']\n",
    "\n",
    "4. 王立芹的email是什么？  \n",
    "深度优化\n",
    "\n",
    "5. 我想发个email给xxx，你能告诉我她的地址吗？？    \n",
    "fault_list: ['崔月猛']\n",
    "\n",
    "6. 你有查找xxx电子邮箱这个查询功能吗？  \n",
    "fault_list: ['李向明', '吴士中', '张信', '曹信福']\n",
    "\n",
    "#### 找职位\n",
    "1. xxx在公司是干什么的？  \n",
    "深度优化\n",
    "\n",
    "2. {}是JAVA开发工程师吗？  \n",
    "深度优化\n",
    "\n",
    "3. xxx的工作职位是什么？  \n",
    "\n",
    "4. xxx在公司承担什么样的工作？  \n",
    "深度优化\n",
    "\n",
    "#### 找部门\n",
    "1. 我想知道{}在用友的那个部门工作\n",
    "\n",
    "2. xxx在那个组？\n",
    "深度优化\n",
    "\n",
    "3. 你的库里友xxx的部门吗？\n",
    "fault_list: ['吴士中', '崔月猛']\n",
    "\n",
    "4. 请帮我找一下{}所属的部门？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-22 15:36:15,860 : INFO : loading Word2Vec object from w2v_model_50_V2\n",
      "2018-06-22 15:36:15,863 : INFO : loading wv recursively from w2v_model_50_V2.wv.* with mmap=None\n",
      "2018-06-22 15:36:15,864 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-22 15:36:15,865 : INFO : loading vocabulary recursively from w2v_model_50_V2.vocabulary.* with mmap=None\n",
      "2018-06-22 15:36:15,866 : INFO : loading trainables recursively from w2v_model_50_V2.trainables.* with mmap=None\n",
      "2018-06-22 15:36:15,868 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-22 15:36:15,868 : INFO : loaded w2v_model_50_V2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'找邮箱'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SemanticIntentionClassify(model_path,para)\n",
    "nn_model = model._load_nn_model()\n",
    "model_w2v, word_to_index = model._load_word2vec_model()\n",
    "x = \"你能告诉我王立芹的工作邮箱吗？\"\n",
    "model.predict(x,nn_model,model_w2v, word_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dptm = pd.read_csv('usr_dict_department')\n",
    "person = pd.read_csv('/home/cuiym/桌面/chat_bot/chatbot_test/chatbot_predict/src/classifier/KG_contact/templates/model/usr_dict_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = list(pd.read_csv('raw_data.csv')['X'])\n",
    "label = list(pd.read_csv('raw_data.csv')['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "#模型预测统计\n",
    "total_fault = {}\n",
    "for i in range(len(sentence)):\n",
    "    print (i)\n",
    "    fault_list = []\n",
    "    for p in list(person['person']):\n",
    "        x = sentence[i].format(p)\n",
    "        y = label[i]\n",
    "        pred_y = model.predict(x,nn_model,model_w2v, word_to_index)\n",
    "        if pred_y == y:\n",
    "            continue\n",
    "        else:\n",
    "            fault_list.append(p)\n",
    "    total_fault[i] = fault_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'找职位'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = '崔月猛的职位？'\n",
    "model.predict(x,nn_model,model_w2v, word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "1 23\n",
      "2 234\n",
      "3 0\n",
      "4 0\n",
      "5 162\n",
      "6 0\n",
      "7 8\n",
      "8 6\n",
      "9 2\n",
      "10 16\n",
      "11 0\n",
      "12 20\n",
      "13 0\n",
      "14 0\n",
      "15 0\n",
      "16 5\n",
      "17 0\n",
      "18 0\n",
      "19 0\n",
      "20 0\n",
      "21 0\n",
      "22 0\n",
      "23 6\n",
      "24 0\n",
      "25 0\n",
      "26 0\n",
      "27 0\n",
      "28 0\n",
      "29 0\n",
      "30 0\n",
      "31 0\n",
      "32 0\n",
      "33 0\n",
      "34 0\n",
      "35 0\n",
      "36 0\n",
      "37 1\n",
      "38 0\n",
      "39 0\n",
      "40 0\n",
      "41 0\n",
      "42 0\n",
      "43 0\n",
      "44 0\n",
      "45 0\n",
      "46 0\n",
      "47 0\n",
      "48 0\n",
      "49 0\n",
      "50 0\n",
      "51 0\n",
      "52 16\n",
      "53 0\n",
      "54 0\n",
      "55 0\n",
      "56 0\n",
      "57 0\n",
      "58 0\n",
      "59 0\n",
      "60 0\n",
      "61 0\n",
      "62 11\n",
      "63 28\n",
      "64 0\n",
      "65 0\n",
      "66 0\n",
      "67 0\n",
      "68 2\n",
      "69 0\n",
      "70 0\n",
      "71 0\n",
      "72 0\n",
      "73 0\n",
      "74 0\n",
      "75 0\n",
      "76 0\n",
      "77 0\n",
      "78 0\n",
      "79 0\n",
      "80 176\n",
      "81 17\n",
      "82 13\n",
      "83 0\n",
      "84 0\n",
      "85 0\n",
      "86 0\n",
      "87 62\n",
      "88 0\n",
      "89 184\n",
      "90 0\n",
      "91 0\n",
      "92 0\n",
      "93 0\n",
      "94 0\n",
      "95 0\n",
      "96 0\n",
      "97 0\n",
      "98 0\n",
      "99 0\n",
      "100 0\n",
      "101 0\n",
      "102 0\n",
      "103 0\n",
      "104 0\n",
      "105 0\n",
      "106 0\n",
      "107 0\n",
      "108 0\n",
      "109 0\n",
      "110 0\n",
      "111 0\n",
      "112 0\n",
      "113 312\n",
      "114 16\n",
      "115 0\n",
      "116 0\n",
      "117 3\n",
      "118 0\n"
     ]
    }
   ],
   "source": [
    "for key,value in total_fault.items():\n",
    "    print (key,len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
